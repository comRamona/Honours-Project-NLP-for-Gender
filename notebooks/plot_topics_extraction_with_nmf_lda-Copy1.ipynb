{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "This is an example of applying :class:`sklearn.decomposition.NMF` and\n",
    ":class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
    "of documents and extract additive models of the topic structure of the\n",
    "corpus.  The output is a list of topics, each represented as a list of\n",
    "terms (weights are not shown).\n",
    "\n",
    "Non-negative Matrix Factorization is applied with two different objective\n",
    "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\n",
    "The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summa.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import environ\n",
    "import os\n",
    "import logging\n",
    "import _pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "import re\n",
    "from metadata import metadata\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import langid\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "rng = np.random.RandomState(10102016)\n",
    "np.random.seed(18101995)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "import datetime\n",
    "\n",
    "def dehyphenate(s):\n",
    "    return s.replace('-\\n','').lower()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def psave(ob, filename, timestamp = False):\n",
    "    timenow = \"\"\n",
    "    if timestamp:\n",
    "        timenow = '{:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    with(open(\"../models/\" + filename + timenow,\"wb\")) as f:\n",
    "        pkl.dump(ob,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metadata.metadata import ACL_metadata\n",
    "acl = ACL_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23595"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl.train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 10000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H89-1036'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl.get_id(acl.train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 4.49 s, total: 1min 43s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os, re\n",
    "# Get all document texts and their corresponding IDs.\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for file in acl.train_files:\n",
    "    doc_ids.append(acl.get_id(file))\n",
    "    with open(file, errors='ignore', encoding='utf-8') as fid:\n",
    "        txt = fid.read()\n",
    "        # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "        txt = re.sub('\\s', ' ', txt)\n",
    "        txt = dehyphenate(txt)    \n",
    "        docs.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23595/23595 [1:00:34<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 30min 52s, sys: 47.6 s, total: 2h 31min 40s\n",
      "Wall time: 1h 29min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(tqdm(docs), n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop and len(token) >= 3]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs\n",
    "\n",
    "import _pickle as pkl\n",
    "with open(\"docs.pkl\",\"wb\") as f:\n",
    "    pkl.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 400, in execute_request\n",
      "    sys.stdout.flush()\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/iostream.py\", line 342, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/threading.py\", line 299, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 400, in execute_request\n",
      "    sys.stdout.flush()\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/ipykernel/iostream.py\", line 342, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/threading.py\", line 551, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/ramona/anaconda3/envs/mlp/lib/python3.6/threading.py\", line 299, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pkl\n",
    "with open(\"docs.pkl\",\"rb\") as f:\n",
    "    pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "PROGRESS: at sentence #10000, processed 22260789 words and 21 word types\n",
      "PROGRESS: at sentence #20000, processed 44689734 words and 25 word types\n",
      "collected 27 word types from a corpus of 52693572 words (unigram + bigrams) and 23595 sentences\n",
      "using 27 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "/home/ramona/anaconda3/envs/mlp/lib/python3.6/site-packages/gensim/models/phrases.py:431: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models.phrases import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding document #0 to Dictionary(0 unique tokens: [])\n",
      "adding document #10000 to Dictionary(7 unique tokens: ['', 'because', 'go', 'shall', 'do']...)\n",
      "adding document #20000 to Dictionary(8 unique tokens: ['', 'because', 'go', 'shall', 'do']...)\n",
      "built Dictionary(8 unique tokens: ['', 'because', 'go', 'shall', 'do']...) from 23595 documents (total 52693572 corpus positions)\n",
      "discarding 6 tokens: [('', 23433), ('shall', 3), ('do', 6), ('nothing', 5), ('something', 3), ('enough', 1)]...\n",
      "keeping 2 tokens which were in no less than 20 and no more than 11797 (=50.0%) documents\n",
      "resulting dictionary: Dictionary(2 unique tokens: ['because', 'go'])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2\n",
      "Number of documents: 23595\n"
     ]
    }
   ],
   "source": [
    "#print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 174.031s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, \n",
    "                                input='filename',\n",
    "                                min_df=7,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\b\",\n",
    "                                preprocessor = dehyphenate\n",
    "                                #tokenizer=TreebankWordTokenizer().tokenize\n",
    "                                )\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(acl.train_files)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "psave(tf_vectorizer,\"tf_vectozier\" + str(n_features), True)\n",
    "psave(tf,\"tf\" + str(n_features),True)\n",
    "\n",
    "# transform sparse matrix into gensim corpus\n",
    "corpus= gensim.matutils.Sparse2Corpus(tf, documents_columns=False)\n",
    "\n",
    "# transform scikit vocabulary into gensim dictionary\n",
    "vocabulary_gensim = {}\n",
    "for key, val in tf_vectorizer.vocabulary_.items():\n",
    "    vocabulary_gensim[val] = key\n",
    "    \n",
    "dic = Dictionary(tf_vectorizer.vocabulary)\n",
    "    \n",
    "psave(vocabulary_gensim,\"vocabulary\" + str(n_features),True)\n",
    "psave(corpus,\"corpus\" + str(n_features),True)\n",
    "psave(dic, \"dic\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for 20 newsgroups. \n",
    "For the 20 newsgroups dataset, we download the articles using\n",
    "the scikit-learn interface, without removing headers, footers or quotes. We parse the text using\n",
    "spaCy6\n",
    "and convert all characters to lower case. Optionally, we then exclude stopwords using the list\n",
    "of standard stopwords in Mallet. We then keep the 2000 words which appear in the largest number of\n",
    "documents.7\n",
    "\n",
    "### Preprocessing for NIPS.\n",
    "For this dataset, we use the same processing as above, except that we use\n",
    "a vocabulary size of 10,000 words, and we exclude all tokens which involve any symbols other than\n",
    "alphabetic characters, and drop all tokens of length less than 3, in order to avoid ambiguous tokens\n",
    "like section numbers and mathematical symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using symmetric alpha at 0.1\n",
      "using symmetric eta at 0.01\n",
      "using serial LDA version on this node\n",
      "running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 23595 documents, updating model once every 2000 documents, evaluating perplexity every 20000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "PROGRESS: pass 0, at document #2000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #3 (0.100): 0.050*\"semantic\" + 0.026*\"lexical\" + 0.023*\"tree\" + 0.023*\"corpus\" + 0.022*\"word\" + 0.020*\"words\" + 0.020*\"model\" + 0.020*\"parsing\" + 0.017*\"proceedings\" + 0.017*\"table\"\n",
      "topic #4 (0.100): 0.037*\"data\" + 0.029*\"semantic\" + 0.026*\"phrase\" + 0.020*\"systems\" + 0.020*\"model\" + 0.020*\"structure\" + 0.018*\"table\" + 0.018*\"evaluation\" + 0.017*\"task\" + 0.017*\"text\"\n",
      "topic #0 (0.100): 0.034*\"feature\" + 0.034*\"text\" + 0.027*\"translation\" + 0.024*\"sentence\" + 0.022*\"features\" + 0.020*\"model\" + 0.019*\"grammar\" + 0.018*\"structure\" + 0.018*\"table\" + 0.017*\"knowledge\"\n",
      "topic #5 (0.100): 0.067*\"word\" + 0.035*\"words\" + 0.029*\"model\" + 0.025*\"features\" + 0.024*\"data\" + 0.023*\"text\" + 0.022*\"sentence\" + 0.020*\"models\" + 0.018*\"proceedings\" + 0.016*\"table\"\n",
      "topic #9 (0.100): 0.033*\"tree\" + 0.031*\"rules\" + 0.027*\"grammar\" + 0.022*\"type\" + 0.021*\"figure\" + 0.021*\"word\" + 0.020*\"rule\" + 0.019*\"dependency\" + 0.019*\"words\" + 0.018*\"sentence\"\n",
      "topic diff=0.493984, rho=1.000000\n",
      "PROGRESS: pass 0, at document #4000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #7 (0.100): 0.070*\"model\" + 0.038*\"features\" + 0.035*\"training\" + 0.035*\"data\" + 0.024*\"models\" + 0.024*\"feature\" + 0.019*\"table\" + 0.019*\"algorithm\" + 0.018*\"word\" + 0.018*\"verb\"\n",
      "topic #8 (0.100): 0.046*\"discourse\" + 0.036*\"text\" + 0.036*\"relations\" + 0.029*\"relation\" + 0.027*\"sentence\" + 0.020*\"corpus\" + 0.020*\"sentences\" + 0.020*\"semantic\" + 0.018*\"type\" + 0.017*\"time\"\n",
      "topic #3 (0.100): 0.081*\"semantic\" + 0.031*\"lexical\" + 0.026*\"parsing\" + 0.025*\"parser\" + 0.023*\"tree\" + 0.021*\"word\" + 0.021*\"terms\" + 0.020*\"syntactic\" + 0.020*\"corpus\" + 0.019*\"words\"\n",
      "topic #6 (0.100): 0.048*\"sentence\" + 0.037*\"sentences\" + 0.027*\"corpus\" + 0.024*\"translation\" + 0.024*\"words\" + 0.023*\"model\" + 0.022*\"word\" + 0.021*\"text\" + 0.019*\"proceedings\" + 0.019*\"english\"\n",
      "topic #9 (0.100): 0.040*\"rules\" + 0.037*\"tree\" + 0.035*\"grammar\" + 0.029*\"rule\" + 0.025*\"parsing\" + 0.023*\"type\" + 0.021*\"structure\" + 0.021*\"figure\" + 0.021*\"dependency\" + 0.020*\"word\"\n",
      "topic diff=0.183030, rho=0.707107\n",
      "PROGRESS: pass 0, at document #6000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #2 (0.100): 0.063*\"word\" + 0.052*\"words\" + 0.049*\"sense\" + 0.039*\"similarity\" + 0.029*\"corpus\" + 0.023*\"target\" + 0.022*\"context\" + 0.021*\"lexical\" + 0.021*\"algorithm\" + 0.020*\"text\"\n",
      "topic #0 (0.100): 0.058*\"feature\" + 0.055*\"text\" + 0.040*\"features\" + 0.031*\"knowledge\" + 0.025*\"translation\" + 0.021*\"sentence\" + 0.021*\"linguistic\" + 0.020*\"structure\" + 0.019*\"level\" + 0.018*\"lexical\"\n",
      "topic #4 (0.100): 0.050*\"data\" + 0.031*\"systems\" + 0.029*\"task\" + 0.025*\"document\" + 0.024*\"evaluation\" + 0.024*\"text\" + 0.021*\"proceedings\" + 0.020*\"table\" + 0.019*\"pages\" + 0.018*\"conference\"\n",
      "topic #6 (0.100): 0.069*\"sentence\" + 0.055*\"sentences\" + 0.033*\"corpus\" + 0.024*\"words\" + 0.022*\"model\" + 0.022*\"text\" + 0.022*\"translation\" + 0.021*\"english\" + 0.020*\"proceedings\" + 0.020*\"word\"\n",
      "topic #9 (0.100): 0.046*\"rules\" + 0.040*\"grammar\" + 0.039*\"tree\" + 0.035*\"rule\" + 0.030*\"parsing\" + 0.025*\"dependency\" + 0.023*\"structure\" + 0.021*\"figure\" + 0.020*\"type\" + 0.019*\"case\"\n",
      "topic diff=0.130185, rho=0.577350\n",
      "PROGRESS: pass 0, at document #8000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #3 (0.100): 0.113*\"semantic\" + 0.038*\"lexical\" + 0.032*\"syntactic\" + 0.031*\"parser\" + 0.029*\"parsing\" + 0.026*\"terms\" + 0.020*\"word\" + 0.019*\"corpus\" + 0.019*\"words\" + 0.018*\"proceedings\"\n",
      "topic #2 (0.100): 0.071*\"word\" + 0.060*\"words\" + 0.051*\"sense\" + 0.043*\"similarity\" + 0.032*\"corpus\" + 0.026*\"context\" + 0.022*\"target\" + 0.022*\"lexical\" + 0.020*\"algorithm\" + 0.019*\"data\"\n",
      "topic #5 (0.100): 0.102*\"word\" + 0.063*\"words\" + 0.042*\"speech\" + 0.039*\"model\" + 0.028*\"features\" + 0.025*\"user\" + 0.024*\"models\" + 0.024*\"data\" + 0.022*\"text\" + 0.018*\"table\"\n",
      "topic #6 (0.100): 0.089*\"sentence\" + 0.075*\"sentences\" + 0.035*\"corpus\" + 0.024*\"words\" + 0.023*\"text\" + 0.021*\"model\" + 0.020*\"proceedings\" + 0.019*\"word\" + 0.019*\"english\" + 0.017*\"translation\"\n",
      "topic #0 (0.100): 0.065*\"text\" + 0.064*\"feature\" + 0.048*\"features\" + 0.034*\"knowledge\" + 0.024*\"linguistic\" + 0.021*\"lexical\" + 0.020*\"structure\" + 0.020*\"level\" + 0.019*\"translation\" + 0.019*\"sentence\"\n",
      "topic diff=0.099525, rho=0.500000\n",
      "PROGRESS: pass 0, at document #10000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #2 (0.100): 0.077*\"word\" + 0.066*\"words\" + 0.049*\"sense\" + 0.048*\"similarity\" + 0.033*\"corpus\" + 0.028*\"context\" + 0.022*\"target\" + 0.021*\"lexical\" + 0.021*\"algorithm\" + 0.019*\"pairs\"\n",
      "topic #0 (0.100): 0.070*\"feature\" + 0.069*\"text\" + 0.054*\"features\" + 0.038*\"knowledge\" + 0.028*\"linguistic\" + 0.022*\"level\" + 0.022*\"lexical\" + 0.021*\"structure\" + 0.016*\"sentence\" + 0.015*\"domain\"\n",
      "topic #8 (0.100): 0.072*\"discourse\" + 0.055*\"relations\" + 0.055*\"relation\" + 0.037*\"text\" + 0.024*\"sentence\" + 0.023*\"structure\" + 0.022*\"type\" + 0.020*\"corpus\" + 0.019*\"time\" + 0.018*\"knowledge\"\n",
      "topic #5 (0.100): 0.113*\"word\" + 0.073*\"words\" + 0.046*\"speech\" + 0.041*\"model\" + 0.025*\"features\" + 0.024*\"data\" + 0.024*\"models\" + 0.022*\"user\" + 0.021*\"text\" + 0.018*\"table\"\n",
      "topic #6 (0.100): 0.108*\"sentence\" + 0.089*\"sentences\" + 0.036*\"corpus\" + 0.024*\"words\" + 0.023*\"text\" + 0.020*\"model\" + 0.020*\"proceedings\" + 0.019*\"english\" + 0.018*\"word\" + 0.017*\"pairs\"\n",
      "topic diff=0.083455, rho=0.447214\n",
      "PROGRESS: pass 0, at document #12000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #1 (0.100): 0.095*\"translation\" + 0.044*\"word\" + 0.041*\"english\" + 0.033*\"model\" + 0.033*\"data\" + 0.031*\"source\" + 0.030*\"machine\" + 0.030*\"words\" + 0.025*\"phrase\" + 0.025*\"target\"\n",
      "topic #6 (0.100): 0.122*\"sentence\" + 0.102*\"sentences\" + 0.036*\"corpus\" + 0.024*\"words\" + 0.023*\"text\" + 0.020*\"proceedings\" + 0.018*\"model\" + 0.017*\"english\" + 0.017*\"word\" + 0.016*\"pairs\"\n",
      "topic #2 (0.100): 0.081*\"word\" + 0.068*\"words\" + 0.053*\"similarity\" + 0.051*\"sense\" + 0.033*\"corpus\" + 0.028*\"context\" + 0.021*\"lexical\" + 0.021*\"target\" + 0.021*\"algorithm\" + 0.019*\"pairs\"\n",
      "topic #4 (0.100): 0.056*\"data\" + 0.034*\"systems\" + 0.033*\"task\" + 0.033*\"document\" + 0.031*\"user\" + 0.028*\"evaluation\" + 0.027*\"text\" + 0.023*\"proceedings\" + 0.021*\"pages\" + 0.021*\"table\"\n",
      "topic #8 (0.100): 0.081*\"discourse\" + 0.062*\"relations\" + 0.061*\"relation\" + 0.034*\"text\" + 0.026*\"structure\" + 0.022*\"sentence\" + 0.022*\"type\" + 0.018*\"time\" + 0.018*\"corpus\" + 0.018*\"knowledge\"\n",
      "topic diff=0.071268, rho=0.408248\n",
      "PROGRESS: pass 0, at document #14000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #1 (0.100): 0.098*\"translation\" + 0.043*\"english\" + 0.043*\"word\" + 0.035*\"model\" + 0.034*\"source\" + 0.033*\"data\" + 0.032*\"machine\" + 0.028*\"phrase\" + 0.028*\"words\" + 0.028*\"target\"\n",
      "topic #6 (0.100): 0.133*\"sentence\" + 0.112*\"sentences\" + 0.038*\"corpus\" + 0.024*\"text\" + 0.024*\"words\" + 0.020*\"proceedings\" + 0.017*\"model\" + 0.016*\"english\" + 0.016*\"table\" + 0.016*\"word\"\n",
      "topic #0 (0.100): 0.079*\"text\" + 0.073*\"feature\" + 0.064*\"features\" + 0.043*\"knowledge\" + 0.029*\"linguistic\" + 0.026*\"level\" + 0.024*\"lexical\" + 0.020*\"structure\" + 0.018*\"type\" + 0.018*\"analysis\"\n",
      "topic #4 (0.100): 0.056*\"data\" + 0.035*\"systems\" + 0.034*\"document\" + 0.034*\"task\" + 0.033*\"user\" + 0.029*\"evaluation\" + 0.028*\"text\" + 0.023*\"proceedings\" + 0.021*\"pages\" + 0.021*\"figure\"\n",
      "topic #5 (0.100): 0.127*\"word\" + 0.081*\"words\" + 0.060*\"speech\" + 0.044*\"model\" + 0.025*\"data\" + 0.024*\"models\" + 0.020*\"features\" + 0.019*\"text\" + 0.019*\"corpus\" + 0.018*\"table\"\n",
      "topic diff=0.059716, rho=0.377964\n",
      "PROGRESS: pass 0, at document #16000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #6 (0.100): 0.144*\"sentence\" + 0.117*\"sentences\" + 0.038*\"corpus\" + 0.025*\"text\" + 0.023*\"words\" + 0.020*\"proceedings\" + 0.016*\"evaluation\" + 0.016*\"table\" + 0.016*\"english\" + 0.016*\"model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "topic #1 (0.100): 0.105*\"translation\" + 0.046*\"english\" + 0.040*\"word\" + 0.036*\"model\" + 0.036*\"source\" + 0.034*\"machine\" + 0.030*\"data\" + 0.030*\"phrase\" + 0.029*\"target\" + 0.027*\"words\"\n",
      "topic #0 (0.100): 0.083*\"text\" + 0.071*\"feature\" + 0.065*\"features\" + 0.044*\"knowledge\" + 0.032*\"linguistic\" + 0.027*\"level\" + 0.024*\"lexical\" + 0.020*\"structure\" + 0.019*\"analysis\" + 0.018*\"type\"\n",
      "topic #2 (0.100): 0.083*\"word\" + 0.070*\"words\" + 0.055*\"similarity\" + 0.053*\"sense\" + 0.034*\"corpus\" + 0.031*\"context\" + 0.022*\"method\" + 0.021*\"target\" + 0.021*\"pairs\" + 0.020*\"lexical\"\n",
      "topic #8 (0.100): 0.080*\"discourse\" + 0.073*\"relations\" + 0.073*\"relation\" + 0.032*\"text\" + 0.027*\"structure\" + 0.024*\"type\" + 0.020*\"sentence\" + 0.019*\"time\" + 0.018*\"knowledge\" + 0.017*\"corpus\"\n",
      "topic diff=0.055122, rho=0.353553\n",
      "PROGRESS: pass 0, at document #18000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #5 (0.100): 0.136*\"word\" + 0.092*\"words\" + 0.068*\"speech\" + 0.045*\"model\" + 0.025*\"models\" + 0.024*\"data\" + 0.021*\"corpus\" + 0.019*\"table\" + 0.019*\"text\" + 0.016*\"features\"\n",
      "topic #3 (0.100): 0.148*\"semantic\" + 0.054*\"syntactic\" + 0.044*\"lexical\" + 0.037*\"verb\" + 0.026*\"parser\" + 0.026*\"noun\" + 0.022*\"terms\" + 0.022*\"parsing\" + 0.018*\"word\" + 0.017*\"corpus\"\n",
      "topic #6 (0.100): 0.154*\"sentence\" + 0.122*\"sentences\" + 0.039*\"corpus\" + 0.025*\"text\" + 0.023*\"words\" + 0.019*\"proceedings\" + 0.016*\"table\" + 0.016*\"evaluation\" + 0.016*\"english\" + 0.015*\"word\"\n",
      "topic #8 (0.100): 0.085*\"discourse\" + 0.077*\"relations\" + 0.076*\"relation\" + 0.031*\"text\" + 0.029*\"structure\" + 0.026*\"type\" + 0.019*\"sentence\" + 0.019*\"time\" + 0.018*\"knowledge\" + 0.017*\"corpus\"\n",
      "topic #7 (0.100): 0.083*\"model\" + 0.062*\"features\" + 0.053*\"training\" + 0.048*\"data\" + 0.039*\"models\" + 0.037*\"learning\" + 0.033*\"feature\" + 0.023*\"table\" + 0.023*\"performance\" + 0.022*\"proceedings\"\n",
      "topic diff=0.048333, rho=0.333333\n",
      "-4.285 per-word bound, 19.5 perplexity estimate based on a held-out corpus of 2000 documents with 821624 words\n",
      "PROGRESS: pass 0, at document #20000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #4 (0.100): 0.053*\"data\" + 0.041*\"user\" + 0.038*\"document\" + 0.036*\"task\" + 0.035*\"systems\" + 0.029*\"evaluation\" + 0.029*\"text\" + 0.024*\"proceedings\" + 0.022*\"pages\" + 0.022*\"figure\"\n",
      "topic #0 (0.100): 0.083*\"text\" + 0.071*\"feature\" + 0.068*\"features\" + 0.046*\"knowledge\" + 0.035*\"linguistic\" + 0.028*\"level\" + 0.025*\"lexical\" + 0.021*\"type\" + 0.021*\"analysis\" + 0.020*\"structure\"\n",
      "topic #6 (0.100): 0.159*\"sentence\" + 0.125*\"sentences\" + 0.040*\"corpus\" + 0.026*\"text\" + 0.023*\"words\" + 0.019*\"proceedings\" + 0.016*\"table\" + 0.016*\"evaluation\" + 0.015*\"score\" + 0.015*\"method\"\n",
      "topic #8 (0.100): 0.088*\"discourse\" + 0.081*\"relation\" + 0.079*\"relations\" + 0.030*\"text\" + 0.029*\"structure\" + 0.027*\"type\" + 0.019*\"sentence\" + 0.018*\"knowledge\" + 0.018*\"time\" + 0.018*\"types\"\n",
      "topic #2 (0.100): 0.085*\"word\" + 0.072*\"words\" + 0.056*\"similarity\" + 0.055*\"sense\" + 0.036*\"corpus\" + 0.032*\"context\" + 0.026*\"method\" + 0.022*\"pairs\" + 0.021*\"target\" + 0.020*\"algorithm\"\n",
      "topic diff=0.044868, rho=0.316228\n",
      "PROGRESS: pass 0, at document #22000/23595\n",
      "merging changes from 2000 documents into a model of 23595 documents\n",
      "topic #6 (0.100): 0.164*\"sentence\" + 0.129*\"sentences\" + 0.041*\"corpus\" + 0.026*\"text\" + 0.022*\"words\" + 0.019*\"proceedings\" + 0.017*\"table\" + 0.016*\"evaluation\" + 0.016*\"score\" + 0.015*\"method\"\n",
      "topic #1 (0.100): 0.116*\"translation\" + 0.054*\"english\" + 0.039*\"source\" + 0.038*\"word\" + 0.037*\"machine\" + 0.037*\"model\" + 0.033*\"phrase\" + 0.032*\"target\" + 0.027*\"data\" + 0.024*\"statistical\"\n",
      "topic #3 (0.100): 0.146*\"semantic\" + 0.056*\"syntactic\" + 0.052*\"verb\" + 0.046*\"lexical\" + 0.035*\"noun\" + 0.022*\"parser\" + 0.020*\"terms\" + 0.019*\"parsing\" + 0.018*\"corpus\" + 0.017*\"word\"\n",
      "topic #4 (0.100): 0.052*\"data\" + 0.044*\"user\" + 0.038*\"document\" + 0.036*\"systems\" + 0.035*\"task\" + 0.030*\"evaluation\" + 0.029*\"text\" + 0.024*\"proceedings\" + 0.022*\"figure\" + 0.022*\"pages\"\n",
      "topic #8 (0.100): 0.091*\"discourse\" + 0.083*\"relations\" + 0.082*\"relation\" + 0.032*\"structure\" + 0.029*\"text\" + 0.027*\"type\" + 0.021*\"time\" + 0.019*\"knowledge\" + 0.018*\"types\" + 0.017*\"sentence\"\n",
      "topic diff=0.042175, rho=0.301511\n",
      "-4.267 per-word bound, 19.2 perplexity estimate based on a held-out corpus of 1595 documents with 663498 words\n",
      "PROGRESS: pass 0, at document #23595/23595\n",
      "merging changes from 1595 documents into a model of 23595 documents\n",
      "topic #3 (0.100): 0.151*\"semantic\" + 0.057*\"syntactic\" + 0.053*\"verb\" + 0.046*\"lexical\" + 0.037*\"noun\" + 0.022*\"parser\" + 0.020*\"terms\" + 0.018*\"corpus\" + 0.018*\"parsing\" + 0.016*\"proceedings\"\n",
      "topic #9 (0.100): 0.057*\"grammar\" + 0.057*\"rules\" + 0.052*\"tree\" + 0.045*\"parsing\" + 0.045*\"rule\" + 0.029*\"structure\" + 0.027*\"parser\" + 0.026*\"case\" + 0.025*\"dependency\" + 0.024*\"figure\"\n",
      "topic #5 (0.100): 0.149*\"word\" + 0.103*\"words\" + 0.069*\"speech\" + 0.050*\"model\" + 0.025*\"models\" + 0.024*\"corpus\" + 0.024*\"data\" + 0.019*\"table\" + 0.017*\"text\" + 0.016*\"training\"\n",
      "topic #4 (0.100): 0.053*\"data\" + 0.043*\"user\" + 0.039*\"document\" + 0.036*\"systems\" + 0.036*\"task\" + 0.029*\"text\" + 0.029*\"evaluation\" + 0.024*\"proceedings\" + 0.022*\"figure\" + 0.022*\"pages\"\n",
      "topic #6 (0.100): 0.169*\"sentence\" + 0.134*\"sentences\" + 0.043*\"corpus\" + 0.026*\"text\" + 0.022*\"words\" + 0.019*\"proceedings\" + 0.017*\"table\" + 0.017*\"evaluation\" + 0.016*\"score\" + 0.016*\"method\"\n",
      "topic diff=0.040301, rho=0.288675\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus, num_topics=100, id2word=vocabulary_gensim, passes=50)\n",
    "psave(lda_model, \"ldamodel-\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"../models/ldamodel2017-11-04 03_49_52\",\"rb\") as f:\n",
    "#     lda_model = pkl.load(f)\n",
    "\n",
    "# with open(\"../models/corpus600002017-11-03 22_37_14\",\"rb\") as f:\n",
    "#     corpus = pkl.load(f)\n",
    "\n",
    "# with open(\"../models/dic2017-11-03 22_37_15\",\"rb\") as f:\n",
    "#     dic = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.081*\"text\" + 0.071*\"feature\" + 0.069*\"features\" + 0.048*\"knowledge\" + 0.037*\"linguistic\" + 0.029*\"level\" + 0.026*\"lexical\" + 0.024*\"type\" + 0.022*\"analysis\" + 0.021*\"domain\"'),\n",
       " (1,\n",
       "  '0.117*\"translation\" + 0.058*\"english\" + 0.040*\"source\" + 0.037*\"machine\" + 0.037*\"word\" + 0.037*\"model\" + 0.033*\"phrase\" + 0.033*\"target\" + 0.027*\"data\" + 0.024*\"statistical\"'),\n",
       " (2,\n",
       "  '0.086*\"word\" + 0.074*\"words\" + 0.059*\"similarity\" + 0.054*\"sense\" + 0.036*\"corpus\" + 0.031*\"context\" + 0.029*\"method\" + 0.024*\"pairs\" + 0.021*\"algorithm\" + 0.020*\"table\"'),\n",
       " (3,\n",
       "  '0.151*\"semantic\" + 0.057*\"syntactic\" + 0.053*\"verb\" + 0.046*\"lexical\" + 0.037*\"noun\" + 0.022*\"parser\" + 0.020*\"terms\" + 0.018*\"corpus\" + 0.018*\"parsing\" + 0.016*\"proceedings\"'),\n",
       " (4,\n",
       "  '0.053*\"data\" + 0.043*\"user\" + 0.039*\"document\" + 0.036*\"systems\" + 0.036*\"task\" + 0.029*\"text\" + 0.029*\"evaluation\" + 0.024*\"proceedings\" + 0.022*\"figure\" + 0.022*\"pages\"'),\n",
       " (5,\n",
       "  '0.149*\"word\" + 0.103*\"words\" + 0.069*\"speech\" + 0.050*\"model\" + 0.025*\"models\" + 0.024*\"corpus\" + 0.024*\"data\" + 0.019*\"table\" + 0.017*\"text\" + 0.016*\"training\"'),\n",
       " (6,\n",
       "  '0.169*\"sentence\" + 0.134*\"sentences\" + 0.043*\"corpus\" + 0.026*\"text\" + 0.022*\"words\" + 0.019*\"proceedings\" + 0.017*\"table\" + 0.017*\"evaluation\" + 0.016*\"score\" + 0.016*\"method\"'),\n",
       " (7,\n",
       "  '0.085*\"model\" + 0.061*\"features\" + 0.057*\"training\" + 0.052*\"data\" + 0.042*\"learning\" + 0.040*\"models\" + 0.032*\"feature\" + 0.024*\"table\" + 0.024*\"performance\" + 0.022*\"proceedings\"'),\n",
       " (8,\n",
       "  '0.093*\"discourse\" + 0.085*\"relations\" + 0.085*\"relation\" + 0.032*\"structure\" + 0.029*\"text\" + 0.028*\"type\" + 0.020*\"time\" + 0.019*\"knowledge\" + 0.018*\"types\" + 0.017*\"sentence\"'),\n",
       " (9,\n",
       "  '0.057*\"grammar\" + 0.057*\"rules\" + 0.052*\"tree\" + 0.045*\"parsing\" + 0.045*\"rule\" + 0.029*\"structure\" + 0.027*\"parser\" + 0.026*\"case\" + 0.025*\"dependency\" + 0.024*\"figure\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(lda_model, corpus, dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
